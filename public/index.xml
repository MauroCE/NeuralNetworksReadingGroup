<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neural Networks Reading Group UoB</title>
    <link>/</link>
    <description>Recent content on Neural Networks Reading Group UoB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jul 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Variational Inference with Normalizing Flows </title>
      <link>/variational-inference-with-normalizing-flows/</link>
      <pubDate>Tue, 28 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/variational-inference-with-normalizing-flows/</guid>
      <description>On Tuesday \(28^{\text{th}}\) of July, Mauro presented Variational Inference with Normalizing Flows by Rezende and Mohamed. Two good review papers are Normalizing Flows for Probabilistic Modeling and Inference, which is more Tutorial in nature, and Normalizing Flows: An Introduction and Review of Current Methods, which is a bit more technical. Slides for the talk are available here.
The abstract is given below:
 The choice of approximate posterior distribution is one of the core problems in variational inference.</description>
    </item>
    
    <item>
      <title>Neural Processes</title>
      <link>/neural-processes/</link>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/neural-processes/</guid>
      <description>On Tuesday \(14^{\text{th}}\) of July, Mingxuan presented Neural Processes and Conditional Neural Processes by Marta Garnelo et al. You can find the notes here.
Abstract for Neural Processes:
 A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference.</description>
    </item>
    
    <item>
      <title>Auto-Encoding Variational Bayes</title>
      <link>/auto-encoding-variational-bayes/</link>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/auto-encoding-variational-bayes/</guid>
      <description>On Tuesday \(7^{\text{th}}\) of July, Mauro Camara Escudero presented Auto-Encoding Variational Bayes. Slides can be found here. The abstract is given below.
 How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case.</description>
    </item>
    
    <item>
      <title>Mining gold from implicit models to improve likelihood-free inference</title>
      <link>/mining-gold-from-implicit-models-to-improve-likelihood-free-inference/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/mining-gold-from-implicit-models-to-improve-likelihood-free-inference/</guid>
      <description>On Tuesday \(30^{\text{th}}\) of June Mark Beaumont presented Mining gold from implicit models to improve likelihood-free inference by Johann Brehmer et al. The abstract is given below:
 Simulators often provide the best description of real-world phenomena. However, the probability density that they implicitly define is often intractable, leading to challenging inverse problems for inference. Recently, a number of techniques have been introduced in which a surrogate for the intractable density is learned, including normalizing flows and density ratio estimators.</description>
    </item>
    
    <item>
      <title>Stein Variational Gradient Descent as Gradient Flow</title>
      <link>/stein-variational-gradient-descent-as-gradient-flow/</link>
      <pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/stein-variational-gradient-descent-as-gradient-flow/</guid>
      <description>On Thursday \(25^{\text{th}}\) of June Song Liu presented Stein Variational Gradient Descent as Gradient Flow. The abstract is given below.
 Stein variational gradient descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate given distributions, based on a gradient-based update that guarantees to optimally decrease the KL divergence within a function space. This paper develops the first theoretical analysis on SVGD. We establish that the empirical measures of the SVGD samples weakly converge to the target distribution, and show that the asymptotic behavior of SVGD is characterized by a nonlinear Fokker-Planck equation known as Vlasov equation in physics.</description>
    </item>
    
    <item>
      <title>Adaptive approximation and estimation of deep neural network to intrinsic dimensionality</title>
      <link>/adaptive-approximation-and-estimation-of-deep-neural-network-to-intrinsic-dimensionality/</link>
      <pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/adaptive-approximation-and-estimation-of-deep-neural-network-to-intrinsic-dimensionality/</guid>
      <description>On Tuesday \(16^{\text{th}}\) of June Patrick Rubin-Delanchy presented Adaptive approximation and estimation of deep neural network to intrinsic dimensionality. The abstract is given below.
 We prove that the performance of deep neural networks (DNNs) is mainly determined by an intrinsic low-dimensionality of covariates. DNNs have been providing an outstanding performance empirically, hence, the theoretical properties of DNNs are actively investigated to understand their mechanism. In particular, the behavior of DNNs with respect to high-dimensional data is one of the most important concerns.</description>
    </item>
    
    <item>
      <title>From optimal transport to generative modeling: the VEGAN cookbook</title>
      <link>/from-optimal-transport-to-generative-modeling-the-vegan-cookbook/</link>
      <pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/from-optimal-transport-to-generative-modeling-the-vegan-cookbook/</guid>
      <description>On Tuesday 9th of June Anthony Lee presented From optimal transport to generative modeling: the VEGAN cookbook. The abstract is given below:
 We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution PX and the latent variable model distribution PG. We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space.</description>
    </item>
    
    <item>
      <title>MetFlow: A New Efficient Method for Bridging the Gap between Markov Chain Monte Carlo and Variational Inference</title>
      <link>/metflow-a-new-efficient-method-for-bridging-the-gap-between-markov-chain-monte-carlo-and-variational-inference/</link>
      <pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/metflow-a-new-efficient-method-for-bridging-the-gap-between-markov-chain-monte-carlo-and-variational-inference/</guid>
      <description>On Tuesday 2nd of June Christophe Andrieu presented MetFlow: A New Efficient Method for Bridging the Gap between Markov Chain Monte Carlo and Variational Inference. A related presentation was given by Andy Wang on Markov Chain Monte Carlo and Variational Inference: Bridging the Gap. The abstract is given below:
 In this contribution, we propose a new computationally efficient method to combine Variational Inference (VI) with Markov Chain Monte Carlo (MCMC).</description>
    </item>
    
    <item>
      <title>Preventing Posterior Collapse with delta-VAEs</title>
      <link>/preventing-posterior-collapse-with-delta-vaes/</link>
      <pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/preventing-posterior-collapse-with-delta-vaes/</guid>
      <description>On Tuesday 26th of May Pierre Aurelien Gilliot presented Preventing Posterior Collapse with delta-VAEs. The main reference paper was the original VAE paper. The abstract is given below.
 Due to the phenomenon of “posterior collapse,” current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires augmenting the objective so it does not only maximize the likelihood of the data.</description>
    </item>
    
    <item>
      <title>Deep Learning for Multi-Scale Changepoint Detection in Multivariate Time Series</title>
      <link>/deep-learning-for-multi-scale-changepoint-detection-in-multivariate-time-series/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/deep-learning-for-multi-scale-changepoint-detection-in-multivariate-time-series/</guid>
      <description>On Tuesday’s 5th of May Sam Tickle has presented Deep Learning for Multi-Scale Changepoint Detection in Multivariate Time Series. The abstract is given below.
 Many real-world time series, such as in health, have changepoints where the system’s structure or parameters change. Since changepoints can indicate critical events such as onset of illness, it is highly important to detect them. However, existing methods for changepoint detection (CPD) often require user-specified models and cannot recognize changes that occur gradually or at multiple time-scales.</description>
    </item>
    
    <item>
      <title>Markov Chain Monte Carlo and Variational Inference: Bridging the Gap</title>
      <link>/markov-chain-monte-carlo-and-variational-inference-bridging-the-gap/</link>
      <pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/markov-chain-monte-carlo-and-variational-inference-bridging-the-gap/</guid>
      <description>On Tuesday \(28^{\text{th}}\) of April Andi Wang presented Markov Chain Monte Carlo and Variational Inference: Bridging the Gap - Tim Salimans. His slides are available here and the abstract is given below.
 Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation.</description>
    </item>
    
    <item>
      <title>Information Bottleneck</title>
      <link>/information-bottleneck/</link>
      <pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/information-bottleneck/</guid>
      <description>On Tuesday \(21^{\text{st}}\) of April Mingxuan Yi talked about the topic of information bottleneck. The main references used were Opening the black box of Deep Neural Networks via Information by Ravid Schwartz-Ziv and Deep Learning and the Information Bottleneck Principle. The abstracts are given below
Opening the black box of Deep Neural Networks via Information
 Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization.</description>
    </item>
    
    <item>
      <title>Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights</title>
      <link>/expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights/</link>
      <pubDate>Tue, 14 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights/</guid>
      <description>On Tuesday \(14^{\text{th}}\) of April I presented Expectation Backpropagation:Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights. The abstract is given below.
 Multilayer Neural Networks (MNNs) are commonly trained using gradient descent-based methods, such as BackPropagation (BP). Inference in probabilistic graphical models is often done using variational Bayes methods, such as Expectation Propagation (EP). We show how an EP based approach can also be used to train deterministic MNNs.</description>
    </item>
    
    <item>
      <title>Partially Exchangeable Networks and Architectures for Learning Summary Statistics in Approximate Bayesian Computation</title>
      <link>/partially-exchangeable-networks-and-architectures-for-learning-summary-statistics-in-approximate-bayesian-computation/</link>
      <pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/partially-exchangeable-networks-and-architectures-for-learning-summary-statistics-in-approximate-bayesian-computation/</guid>
      <description>On Tuesday 7\(^{\text{th}}\) of April Mark Beaumont presented Partially Exchangeable Networks and Architectures for Learning Summary Statistics in Approximate Bayesian Computation. The abstract is given below.
 We present a novel family of deep neural architectures, named partially exchangeable networks (PENs) that leverage probabilistic symmetries. By design, PENs are invariant to block-switch transformations, which characterize the partial exchangeability properties of conditionally Markovian processes. Moreover, we show that any block-switch invariant function has a PEN-like representation.</description>
    </item>
    
    <item>
      <title>Neural Ordinary Differential Equations</title>
      <link>/neural-ordinary-differential-equations/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/neural-ordinary-differential-equations/</guid>
      <description>On Tuesday 24\(^{th}\) of March Song Liu presented Neural Ordinary Differential Equations. The abstract is given below.
 We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed.</description>
    </item>
    
    <item>
      <title>Uniform convergence may be unable to explain generalization in deep learning</title>
      <link>/uniform-convergence-may-be-unable-to-explain-generalization-in-deep-learning/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/uniform-convergence-may-be-unable-to-explain-generalization-in-deep-learning/</guid>
      <description>On Tuesday 10\(^{\text{th}}\) of March Patrick Rubin-Delanchy presented the 2019 NeurIPS paper winner of the Outstanding New Directions Paper Award, titled “Uniform convergence may be unable to explain generalization in deep learning” by Vaishnavh Nagarajan and J. Zico Kolter.
The abstract is given below.
 Aimed at explaining the surprisingly good generalization behavior of overparameterized deep networks, recent works have developed a variety of generalization bounds for deep learning, all based on the fundamental learning-theoretic technique of uniform convergence.</description>
    </item>
    
    <item>
      <title>Nonparametric regression using deep neural networks with ReLU activation function</title>
      <link>/nonparametric-regression-using-deep-neural-networks-with-relu-activation-function/</link>
      <pubDate>Thu, 27 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/nonparametric-regression-using-deep-neural-networks-with-relu-activation-function/</guid>
      <description>On Tuesday 17th of December Anthony Lee presented the paper Nonparametric regression using deep neural networks with ReLU activation function by Johannes Schmidt-Hieber. The abstract is given below.
 Consider the multivariate nonparametric regression model. It is shown that estimators based on sparsely connected deep neural networks with ReLU activation function and properly chosen network architecture achieve the minimax rates of convergence (up to logn-factors) under a general composition assumption on the regression function.</description>
    </item>
    
    <item>
      <title>Deep Convolutional Networks</title>
      <link>/deep-convolutional-networks/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/deep-convolutional-networks/</guid>
      <description>On Tuesday 10th of December we watched the video “Multiscale Models for Image Classification and Physics with Deep Networks” by Stéphane Mallat.</description>
    </item>
    
    <item>
      <title>Deep Boltzmann Machines</title>
      <link>/deep-boltzmann-machines/</link>
      <pubDate>Mon, 02 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/deep-boltzmann-machines/</guid>
      <description>On Tuesday 3rd of December Pierre presented the paper Deep Bolzmann Machines by Ruslan Salakhutdinov and Geoffrey Hinton. The slides are available.
The abstract is given below.
 We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters.</description>
    </item>
    
    <item>
      <title>Welcome to Neural Networks reading group website!</title>
      <link>/welcome/</link>
      <pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/welcome/</guid>
      <description>Hi everyone! I hope you are as excited as we are about the Neural Network reading group. We thought we’d make it more engaging by having a simple website where we can share links to the papers, post summaries and ideas. This is still a work in progress and feeback and suggestions are very much appreciated. For now, you can send your feedback to m.camaraescudero@bristol.ac.uk or to ys18223@bristol.ac.uk.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Thu, 05 May 2016 21:48:51 -0700</pubDate>
      
      <guid>/about/</guid>
      <description>This website will be used as a repository of ideas, materials and discussions stemming out of the Neural Networks reading group at the University of Bristol. The reading group is organized by Mauro Camara Escudero and Pierre-Aurelien Gilliot.</description>
    </item>
    
    <item>
      <title>Members of the Neural Network reading group</title>
      <link>/members/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/members/</guid>
      <description> The current members of the Neural Network Reading Group are listed below.
  Members Presenting    Alexander Modell TRUE  Andi Wang TRUE  Andrea Becsek TRUE  Anthony Lee TRUE  Chang Zhang TRUE  Christophe Andrieu TRUE  Hamza Alawiye TRUE  Henry Reeve FALSE  Jennifer Chakravarty FALSE  Mark Beaumont TRUE  Mathieu Gerber TRUE  Mauro Camara Escudero TRUE  Mingxuan Yi TRUE  Nick Whiteley FALSE  Patrick Rubin-Delanchy TRUE  Pierre-Aurelien Gilliot TRUE  Robert Allison TRUE  Sam Tickle TRUE  Song Liu TRUE    </description>
    </item>
    
    <item>
      <title>Neural Network Reading Group Location</title>
      <link>/room/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/room/</guid>
      <description>The Neural Network reading group meets every Tuesday from 10:30 to 11:30 in room 1.11 in the Fry Building (due to COVID we now meet on ZOOM).</description>
    </item>
    
    <item>
      <title>Neural Network Reading Suggestions</title>
      <link>/suggestions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/suggestions/</guid>
      <description>input[type=text], select { width: 100%; padding: 12px 20px; margin: 8px 0; display: inline-block; border: 1px solid #ccc; border-radius: 4px; box-sizing: border-box; } input[type=submit] { width: 100%; background-color: #4CAF50; color: white; padding: 14px 20px; margin: 8px 0; border: none; border-radius: 4px; cursor: pointer; } input[type=submit]:hover { background-color: #45a049; } div { border-radius: 5px; background-color: #f2f2f2; padding: 20px; }  var submitted=false;  Name* Topic of Interest* Suggested Reading Material* Send   The current reading suggestions for the Neural Network reading group are avaiable in this Google Sheet.</description>
    </item>
    
    <item>
      <title>Past Papers</title>
      <link>/papers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/papers/</guid>
      <description>Here’s a list of the papers/videos that have previously been presented.
 table, th, td { border: 1px solid black; border-collapse: collapse; } th, td { padding: 15px; }     Date   Presented By   Paper Title     3rd December   Pierre-Aurelien Gilliot   Deep Boltzmann Machines - Ruslan Salakhutdinov     10th December   Christophe Andrieu   Multiscale Models for Image Classification and Physics with Deep Networks - Stéphane Mallat     17th December   Anthony Lee   Nonparametric regression using deep neural networks with ReLU activation function - Johannes Schmidt-Hieber     10th March   Patrick Rubin-Delanchy   Uniform convergence may be unable to explain generalization in deep learning - Vaishnavh Nagarajan     24th March   Song Liu   Neural Ordinary Differential Equations - Ricky T.</description>
    </item>
    
    <item>
      <title>Reading Group Schedule</title>
      <link>/schedule/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/schedule/</guid>
      <description> The current schedule for the Neural Network reading group is
  Date Name Room    2020-10-13 Chang Zhang ZOOM  2020-10-20 Henry Reeve ZOOM  2020-10-27 Andi Wang ZOOM  2020-11-03 Christophe Andrieu ZOOM  2020-11-10 Alexander Modell ZOOM  2020-11-17 Nick Whiteley ZOOM  2020-11-24 Anthony Lee ZOOM  2020-12-01 Song Liu ZOOM  2020-12-08 Mark Beaumont ZOOM  2020-12-15 Mathieu Gerber ZOOM  2020-02-02 Mauro Camara Escudero ZOOM  2020-02-09 Pierre-Aurelien Gilliot ZOOM  2020-02-16 Mingxuan Yi ZOOM  2020-02-23 Sam Tickle ZOOM  2020-03-01 Patrick Rubin-Delanchy ZOOM  2020-03-08 Robert Allison ZOOM    -- -- -- -- Date -- Name -- -- -- 17th December -- Anthony Lee -- -- -- 14th January -- Song Liu -- -- -- 21st January -- Mark Beaumont -- -- -- 28th January -- Mauro Camara Escudero -- -- -- 4th February -- Patrick Rubin-Delanchy -- -- -- 11th February -- Andi Wang -- -- -- 18th February -- Mathieu Gerber -- -- -- 25th February -- Nick Whiteley -- -- -- </description>
    </item>
    
    <item>
      <title>Thank you!</title>
      <link>/thankyou/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/thankyou/</guid>
      <description>Your response has been submitted!</description>
    </item>
    
  </channel>
</rss>