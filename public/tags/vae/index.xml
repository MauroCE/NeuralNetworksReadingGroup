<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>vae on Neural Networks Reading Group UoB</title>
    <link>/tags/vae/</link>
    <description>Recent content in vae on Neural Networks Reading Group UoB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Jul 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/vae/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Auto-Encoding Variational Bayes</title>
      <link>/auto-encoding-variational-bayes/</link>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/auto-encoding-variational-bayes/</guid>
      <description>On Tuesday \(7^{\text{th}}\) of July, Mauro Camara Escudero presented Auto-Encoding Variational Bayes. Slides can be found here. The abstract is given below.
 How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case.</description>
    </item>
    
    <item>
      <title>From optimal transport to generative modeling: the VEGAN cookbook</title>
      <link>/from-optimal-transport-to-generative-modeling-the-vegan-cookbook/</link>
      <pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/from-optimal-transport-to-generative-modeling-the-vegan-cookbook/</guid>
      <description>On Tuesday 9th of June Anthony Lee presented From optimal transport to generative modeling: the VEGAN cookbook. The abstract is given below:
 We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution PX and the latent variable model distribution PG. We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space.</description>
    </item>
    
  </channel>
</rss>