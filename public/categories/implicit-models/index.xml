<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>implicit-models on Neural Networks Reading Group UoB</title>
    <link>/categories/implicit-models/</link>
    <description>Recent content in implicit-models on Neural Networks Reading Group UoB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Nov 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/implicit-models/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Likelihood-free MCMC with Amortized Approximate Ratio Estimators</title>
      <link>/likelihood-free-mcmc-with-amortized-approximate-ratio-estimators/</link>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/likelihood-free-mcmc-with-amortized-approximate-ratio-estimators/</guid>
      <description>On Monday \(2^{\text{nd}}\) of November, Song presented Likelihood-free MCMC with Amortized Approximate Ratio Estimators. The abstract is given below:
 Posterior inference with an intractable likelihood is becoming an increasingly common task in scientific domains which rely on sophisticated computer simulations. Typically, these forward models do not admit tractable densities forcing practitioners to make use of approximations. This work introduces a novel approach to address the intractability of the likelihood and the marginal model.</description>
    </item>
    
    <item>
      <title>Learning in Implicit Generative Models</title>
      <link>/learning-in-implicit-generative-models/</link>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/learning-in-implicit-generative-models/</guid>
      <description>On Monday the 19th of October 2020 Chang Zhang presented Learning in Implicit Generative Models by Shakir Mohamed and Balaji Lakshminarayanan. The abstract is given below.
 Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers.</description>
    </item>
    
    <item>
      <title>Mining gold from implicit models to improve likelihood-free inference</title>
      <link>/mining-gold-from-implicit-models-to-improve-likelihood-free-inference/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/mining-gold-from-implicit-models-to-improve-likelihood-free-inference/</guid>
      <description>On Tuesday \(30^{\text{th}}\) of June Mark Beaumont presented Mining gold from implicit models to improve likelihood-free inference by Johann Brehmer et al.Â The abstract is given below:
 Simulators often provide the best description of real-world phenomena. However, the probability density that they implicitly define is often intractable, leading to challenging inverse problems for inference. Recently, a number of techniques have been introduced in which a surrogate for the intractable density is learned, including normalizing flows and density ratio estimators.</description>
    </item>
    
  </channel>
</rss>