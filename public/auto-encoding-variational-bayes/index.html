<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.84.1" />


<title>Auto-Encoding Variational Bayes - Neural Networks Reading Group UoB</title>
<meta property="og:title" content="Auto-Encoding Variational Bayes - Neural Networks Reading Group UoB">


  <link href='https://neuralnetworksbristol.netlify.app/favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/room/">Location</a></li>
    
    <li><a href="/members/">Members</a></li>
    
    <li><a href="/papers/">Past Papers</a></li>
    
    <li><a href="/suggestions/">Reading Suggestions</a></li>
    
    <li><a href="/schedule/">Schedule</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">1 min read</span>
    

    <h1 class="article-title">Auto-Encoding Variational Bayes</h1>

    
    <span class="article-date">2020-07-07</span>
    

    <div class="article-content">
      


<p>On Tuesday <span class="math inline">\(7^{\text{th}}\)</span> of July, Mauro Camara Escudero presented <a href="https://arxiv.org/pdf/1312.6114.pdf">Auto-Encoding Variational Bayes</a>. Slides can be found <a href="https://neuralnetworksbristol.netlify.com/VAE_Mauro.pdf">here</a>. The abstract is given below.</p>
<blockquote>
<p>How can we perform efficient inference and learning in directed probabilistic
models, in the presence of continuous latent variables with intractable posterior
distributions, and large datasets? We introduce a stochastic variational inference
and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is
two-fold. First, we show that a reparameterization of the variational lower bound
yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with
continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.
Theoretical advantages are reflected in experimental results.</p>
</blockquote>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-123451981-4', 'auto');
	
	ga('send', 'pageview');
}
</script>
  </body>
</html>

