---
title: Stochastic Normalizing Flows
author: Mauro Camara Escudero
date: '2020-12-14'
slug: stochastic-normalizing-flows
categories:
  - normalizing flows
  - stochastic normalizing flows
tags:
  - stochastic normalizing flows
  - normalizing flows
  - density estimation
  - neural-networks
---
On Monday $14^{\text{th}}$ of December Anthony presented [Stochastic Normalizing Flows](https://arxiv.org/pdf/2002.06707.pdf). The abstract is given below: 

> The sampling of probability distributions specified up to a normalization constant
is an important problem in both machine learning and statistical mechanics. While
classical stochastic sampling methods such as Markov Chain Monte Carlo (MCMC)
or Langevin Dynamics (LD) can suffer from slow mixing times there is a growing
interest in using normalizing flows in order to learn the transformation of a simple
prior distribution to the given target distribution. Here we propose a generalized
and combined approach to sample target densities: Stochastic Normalizing Flows
(SNF) – an arbitrary sequence of deterministic invertible functions and stochastic
sampling blocks. We show that stochasticity overcomes expressivity limitations
of normalizing flows resulting from the invertibility constraint, whereas trainable
transformations between sampling steps improve efficiency of pure MCMC/LD
along the flow. By invoking ideas from non-equilibrium statistical mechanics
we derive an efficient training procedure by which both the sampler’s and the
flow’s parameters can be optimized end-to-end, and by which we can compute
exact importance weights without having to marginalize out the randomness of the
stochastic blocks. We illustrate the representational power, sampling efficiency and
asymptotic correctness of SNFs on several benchmarks including applications to
sampling molecular systems in equilibrium.