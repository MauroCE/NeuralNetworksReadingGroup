<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>vae on Neural Networks Reading Group UoB</title>
    <link>/categories/vae/</link>
    <description>Recent content in vae on Neural Networks Reading Group UoB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jul 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/vae/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Variational Inference with Normalizing Flows </title>
      <link>/variational-inference-with-normalizing-flows/</link>
      <pubDate>Tue, 28 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/variational-inference-with-normalizing-flows/</guid>
      <description>On Tuesday \(28^{\text{th}}\) of July, Mauro presented Variational Inference with Normalizing Flows by Rezende and Mohamed. Two good review papers are Normalizing Flows for Probabilistic Modeling and Inference, which is more Tutorial in nature, and Normalizing Flows: An Introduction and Review of Current Methods, which is a bit more technical. Slides for the talk are available here.
The abstract is given below:
 The choice of approximate posterior distribution is one of the core problems in variational inference.</description>
    </item>
    
    <item>
      <title>Auto-Encoding Variational Bayes</title>
      <link>/auto-encoding-variational-bayes/</link>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/auto-encoding-variational-bayes/</guid>
      <description>On Tuesday \(7^{\text{th}}\) of July, Mauro Camara Escudero presented Auto-Encoding Variational Bayes. Slides can be found here. The abstract is given below.
 How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case.</description>
    </item>
    
    <item>
      <title>From optimal transport to generative modeling: the VEGAN cookbook</title>
      <link>/from-optimal-transport-to-generative-modeling-the-vegan-cookbook/</link>
      <pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/from-optimal-transport-to-generative-modeling-the-vegan-cookbook/</guid>
      <description>On Tuesday 9th of June Anthony Lee presented From optimal transport to generative modeling: the VEGAN cookbook. The abstract is given below:
 We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution PX and the latent variable model distribution PG. We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space.</description>
    </item>
    
    <item>
      <title>Preventing Posterior Collapse with delta-VAEs</title>
      <link>/preventing-posterior-collapse-with-delta-vaes/</link>
      <pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/preventing-posterior-collapse-with-delta-vaes/</guid>
      <description>On Tuesday 26th of May Pierre Aurelien Gilliot presented Preventing Posterior Collapse with delta-VAEs. The main reference paper was the original VAE paper. The abstract is given below.
 Due to the phenomenon of “posterior collapse,” current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires augmenting the objective so it does not only maximize the likelihood of the data.</description>
    </item>
    
  </channel>
</rss>