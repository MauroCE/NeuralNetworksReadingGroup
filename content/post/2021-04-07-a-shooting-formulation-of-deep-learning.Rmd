---
title: A Shooting Formulation of Deep Learning
author: Mauro Camara Escudero
date: '2021-04-07'
slug: a-shooting-formulation-of-deep-learning
categories:
  - neural-ode
  - neural-odes
tags:
  - neural-ode
  - neural-odes
  - neural-networks
  - neural-networks-reading-group
  - deep-learning
  - machine-learning
---
On Wednesday the $7^{\text{th}}$ of April, Anthony presented [A Shooting Formulation of Deep Learning](https://arxiv.org/pdf/2006.10330.pdf). This is closely related to [Neural ODEs](https://arxiv.org/abs/1806.07366) and [Augmented Neural ODEs](https://arxiv.org/abs/1904.01681). The abstract is given below:

> A residual network may be regarded as a discretization of an ordinary differential
equation (ODE) which, in the limit of time discretization, defines a continuous-depth
network. Although important steps have been taken to realize the advantages of such
continuous formulations, most current techniques assume identical layers. Indeed, existing
works throw into relief the myriad difficulties of learning an infinite-dimensional parameter
in a continuous-depth neural network. To this end, we introduce a shooting formulation
which shifts the perspective from parameterizing a network layer-by-layer to parameterizing
over optimal networks described only by a set of initial conditions. For scalability, we
propose a novel particle-ensemble parameterization which fully specifies the optimal
weight trajectory of the continuous-depth neural network. Our experiments show that
our particle-ensemble shooting formulation can achieve competitive performance. Finally,
though the current work is inspired by continuous-depth neural networks, the particleensemble shooting formulation also applies to discrete-time networks and may lead to a
new fertile area of research in deep learning parameterization.