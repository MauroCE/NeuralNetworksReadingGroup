<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>energy-based models on Neural Networks Reading Group UoB</title>
    <link>https://neuralnetworksbristol.netlify.app/tags/energy-based-models/</link>
    <description>Recent content in energy-based models on Neural Networks Reading Group UoB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://neuralnetworksbristol.netlify.app/tags/energy-based-models/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to Train your Energy-Based Models </title>
      <link>https://neuralnetworksbristol.netlify.app/how-to-train-your-energy-based-models/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://neuralnetworksbristol.netlify.app/how-to-train-your-energy-based-models/</guid>
      <description>On Wednesday teh \(24^{\text{th}}\) of February Sam presented How to train your energy-based models by Yang Song and Diederik P. Kingma. You can find his presentation here and the abstract below.
 Energy-Based Models (EBMs), also known as non-normalized probabilistic models, specify probability density or mass functions up to an unknown normalizing constant. Unlike most other probabilistic models, EBMs do not place a restriction on the tractability of the normalizing constant, thus are more flexible to parameterize and can model a more expressive family of probability distributions.</description>
    </item>
    
  </channel>
</rss>
