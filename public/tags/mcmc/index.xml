<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mcmc on Neural Networks Reading Group UoB</title>
    <link>https://neuralnetworksbristol.netlify.app/tags/mcmc/</link>
    <description>Recent content in mcmc on Neural Networks Reading Group UoB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://neuralnetworksbristol.netlify.app/tags/mcmc/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Likelihood-free MCMC with Amortized Approximate Ratio Estimators</title>
      <link>https://neuralnetworksbristol.netlify.app/likelihood-free-mcmc-with-amortized-approximate-ratio-estimators/</link>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://neuralnetworksbristol.netlify.app/likelihood-free-mcmc-with-amortized-approximate-ratio-estimators/</guid>
      <description>On Monday \(2^{\text{nd}}\) of November, Song presented Likelihood-free MCMC with Amortized Approximate Ratio Estimators. The abstract is given below:
 Posterior inference with an intractable likelihood is becoming an increasingly common task in scientific domains which rely on sophisticated computer simulations. Typically, these forward models do not admit tractable densities forcing practitioners to make use of approximations. This work introduces a novel approach to address the intractability of the likelihood and the marginal model.</description>
    </item>
    
    <item>
      <title>MetFlow: A New Efficient Method for Bridging the Gap between Markov Chain Monte Carlo and Variational Inference</title>
      <link>https://neuralnetworksbristol.netlify.app/metflow-a-new-efficient-method-for-bridging-the-gap-between-markov-chain-monte-carlo-and-variational-inference/</link>
      <pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://neuralnetworksbristol.netlify.app/metflow-a-new-efficient-method-for-bridging-the-gap-between-markov-chain-monte-carlo-and-variational-inference/</guid>
      <description>On Tuesday 2nd of June Christophe Andrieu presented MetFlow: A New Efficient Method for Bridging the Gap between Markov Chain Monte Carlo and Variational Inference. A related presentation was given by Andy Wang on Markov Chain Monte Carlo and Variational Inference: Bridging the Gap. The abstract is given below:
 In this contribution, we propose a new computationally efficient method to combine Variational Inference (VI) with Markov Chain Monte Carlo (MCMC).</description>
    </item>
    
    <item>
      <title>Markov Chain Monte Carlo and Variational Inference: Bridging the Gap</title>
      <link>https://neuralnetworksbristol.netlify.app/markov-chain-monte-carlo-and-variational-inference-bridging-the-gap/</link>
      <pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://neuralnetworksbristol.netlify.app/markov-chain-monte-carlo-and-variational-inference-bridging-the-gap/</guid>
      <description>On Tuesday \(28^{\text{th}}\) of April Andi Wang presented Markov Chain Monte Carlo and Variational Inference: Bridging the Gap - Tim Salimans. His slides are available here and the abstract is given below.
 Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation.</description>
    </item>
    
  </channel>
</rss>
