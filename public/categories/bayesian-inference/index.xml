<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bayesian-inference on Neural Networks Reading Group UoB</title>
    <link>/categories/bayesian-inference/</link>
    <description>Recent content in bayesian-inference on Neural Networks Reading Group UoB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Jun 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/bayesian-inference/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Stein Variational Gradient Descent as Gradient Flow</title>
      <link>/stein-variational-gradient-descent-as-gradient-flow/</link>
      <pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/stein-variational-gradient-descent-as-gradient-flow/</guid>
      <description>On Thursday \(25^{\text{th}}\) of June Song Liu presented Stein Variational Gradient Descent as Gradient Flow. The abstract is given below.
 Stein variational gradient descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate given distributions, based on a gradient-based update that guarantees to optimally decrease the KL divergence within a function space. This paper develops the first theoretical analysis on SVGD. We establish that the empirical measures of the SVGD samples weakly converge to the target distribution, and show that the asymptotic behavior of SVGD is characterized by a nonlinear Fokker-Planck equation known as Vlasov equation in physics.</description>
    </item>
    
  </channel>
</rss>