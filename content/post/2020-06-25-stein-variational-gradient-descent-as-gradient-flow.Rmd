---
title: Stein Variational Gradient Descent as Gradient Flow
author: Mauro Camara Escudero
date: '2020-06-25'
slug: stein-variational-gradient-descent-as-gradient-flow
categories:
  - variational-inference
  - gradient-descent
  - bayesian-inference
  - kl-divergence
tags:
  - variaitonal inference
  - gradient-descent
  - bayesian-inference
  - kl-divergence
---

On Thursday $25^{\text{th}}$ of June Song Liu presented [Stein Variational Gradient Descent as Gradient Flow](https://arxiv.org/pdf/1704.07520.pdf). The abstract is given below.

> Stein variational gradient descent (SVGD) is a deterministic sampling algorithm
that iteratively transports a set of particles to approximate given distributions, based
on a gradient-based update that guarantees to optimally decrease the KL divergence
within a function space. This paper develops the first theoretical analysis on SVGD.
We establish that the empirical measures of the SVGD samples weakly converge
to the target distribution, and show that the asymptotic behavior of SVGD is
characterized by a nonlinear Fokker-Planck equation known as Vlasov equation
in physics. We develop a geometric perspective that views SVGD as a gradient
flow of the KL divergence functional under a new metric structure on the space of
distributions induced by Stein operator.
