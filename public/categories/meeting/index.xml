<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Meeting on Neural Networks Reading Group UoB</title>
    <link>/categories/meeting/</link>
    <description>Recent content in Meeting on Neural Networks Reading Group UoB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Feb 2020 00:00:00 +0000</lastBuildDate><atom:link href="/categories/meeting/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Nonparametric regression using deep neural networks with ReLU activation function</title>
      <link>/nonparametric-regression-using-deep-neural-networks-with-relu-activation-function/</link>
      <pubDate>Thu, 27 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/nonparametric-regression-using-deep-neural-networks-with-relu-activation-function/</guid>
      <description>On Tuesday 17th of December Anthony Lee presented the paper Nonparametric regression using deep neural networks with ReLU activation function by Johannes Schmidt-Hieber. The abstract is given below.
 Consider the multivariate nonparametric regression model. It is shown that estimators based on sparsely connected deep neural networks with ReLU activation function and properly chosen network architecture achieve the minimax rates of convergence (up to logn-factors) under a general composition assumption on the regression function.</description>
    </item>
    
    <item>
      <title>Deep Convolutional Networks</title>
      <link>/deep-convolutional-networks/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/deep-convolutional-networks/</guid>
      <description>On Tuesday 10th of December we watched the video “Multiscale Models for Image Classification and Physics with Deep Networks” by Stéphane Mallat.</description>
    </item>
    
    <item>
      <title>Deep Boltzmann Machines</title>
      <link>/deep-boltzmann-machines/</link>
      <pubDate>Mon, 02 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/deep-boltzmann-machines/</guid>
      <description>On Tuesday 3rd of December Pierre presented the paper Deep Bolzmann Machines by Ruslan Salakhutdinov and Geoffrey Hinton. The slides are available.
The abstract is given below.
 We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters.</description>
    </item>
    
  </channel>
</rss>
